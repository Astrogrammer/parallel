# Инструкция по использованию суперкомпьютера (кластера) ВГУ
## Содержание
1. [Как подключиться](#connect)
    - [На Linux через терминал](#linux_term)
    - [На Windows через терминал](#windows_term)
    - [На Windows через PuTTY](#windows_putty)
    - [На Linux и Windows через Visual Studio Code](#vs_code)
2. [Как обмениваться файлами](#files)
    - [На Linux через проводник](#linux_caja)
    - [На Linux через терминал](#linux_scp)
    - [На Windows через WinSCP](#windows_scp)
3. [Как загружать модули](#modules)
    - [Для временного использования](#temp_modules)
    - [Для постоянного использования](#const_modules)
4. [Как запускать код](#start_code)
    - [Компиляция на Си](#c_compilation)
    - [Компиляция на Си с использованием MPI](#c_mpi_compilation)
    - [Постановка в очередь исполнения](#slurm)
5. [Как работать с Xeon Phi](#xeon_phi)
    - [Получение доступа к сопроцессорам](#access_phi)
    - [Компиляция с использованием OpenMP](#omp_phi)
    - [Компиляция с использованием MPI](#mpi_phi)

## <a id="connect">Часть 1. Как подключиться</a>
Прежде чем пытаться установить подключение с кластером, убедитесь, что вам предоставлен к нему доступ. Для этого, как минимум, необходимо  
    а) распечатать и заполнить **заявление на регистрацию**;  
    б) сгенерировать и отправить **публичный SSH-ключ**.  

В случае, если оба пункта выполнены, в ближайшее время можно пробовать установить соединение. Для этого есть несколько основных способов.

### <a id="linux_term">На Linux через терминал</a>
1. Исполните команду `ssh login@hpc.cs.vsu.ru`, где `login` - ваши фамилия_ио **латиницей** (в точном соответствии с вашим заявлением).
2. При успешном подключении в конце приветственного текста вы увидите приглашение ко вводу: `login@head:~> `
3. Введите `mc` (Midnight Commander) для открытия файлового менеджера. Через него вы можете видеть все ваши файлы и перемещаться между каталогами. Для выхода нажмите `F10`.
4. Кроме того, доступны следующие опции:  
   `F3` - открыть файл на чтение  
   `F4` - открыть файл на редактирование  
   `F5` - скопировать файл из одного окна в другое  
   `F6` - переместить файл из одного окна в другое  
   `F8` - удалить файл  
5. Все доступные опции подписаны снизу. При открытии файла на запись вы тоже увидите список возможных команд. В частности, `F2` отвечает за сохранение файла.

### <a id="windows_term">На Windows через терминал</a>
1. В случае, если ваш SSH-ключ был сгенерирован командой `ssh-keygen -t rsa`, откройте терминал через `Win+X` ➔ `Терминал`
2. Исполните команду `ssh login@hpc.cs.vsu.ru`, где `login` - ваши фамилия_ио **латиницей** (в точном соответствии с вашим заявлением).
3. При успешном подключении в конце приветственного текста вы увидите приглашение ко вводу: `login@head:~> `
4. Введите `mc` (Midnight Commander) для открытия файлового менеджера. Через него вы можете видеть все ваши файлы и перемещаться между каталогами. Для выхода нажмите `F10`.
5. Кроме того, доступны следующие опции:  
   `F3` - открыть файл на чтение  
   `F4` - открыть файл на редактирование  
   `F5` - скопировать файл из одного окна в другое  
   `F6` - переместить файл из одного окна в другое  
   `F8` - удалить файл  
6. Все доступные опции подписаны снизу. При открытии файла на запись вы тоже увидите список возможных команд. В частности, `F2` отвечает за сохранение файла.

### <a id="windows_putty">На Windows через PuTTY</a>
1. В случае, если ваш SSH-ключ был сгенерирован программой *puttygen*, запустите программу *pageant*. Появится значок в трее, нажмите на него ПКМ. Выберите *Add key*. Выберите файл **приватного ключа**.
2. Запустите программу *PuTTY*. В поле *host* введите `hpc.cs.vsu.ru`. В поле под "Saved Sessions" введите свой логин формата `фамилия_ио` **латиницей** (в точном соответствии с вашим заявлением). Нажмите *Save* для сохранения учётных данных.
3. Нажмите `Open`. Для несохранённого пользователя откроется окно авторизации. Введите свой логин вышеприведённого формата, нажмите *Enter*. **Перед тем, как вводить первый символ**, убедитесь, что используется английская раскладка клавиатуры (иначе может выдать ошибку при верных данных).
4. При успешном подключении в конце приветственного текста вы увидите приглашение ко вводу: `login@head:~> `
5. Введите `mc` (Midnight Commander) для открытия файлового менеджера. Через него вы можете видеть все ваши файлы и перемещаться между каталогами. Для выхода нажмите `F10`.
6. Кроме того, доступны следующие опции:  
   `F3` - открыть файл на чтение  
   `F4` - открыть файл на редактирование  
   `F5` - скопировать файл из одного окна в другое  
   `F6` - переместить файл из одного окна в другое  
   `F8` - удалить файл  
7. Все доступные опции подписаны снизу. При открытии файла на запись вы тоже увидите список возможных команд. В частности, `F2` отвечает за сохранение файла.

### <a id="vs_code">На Linux и Windows через Visual Studio Code</a>
1. Установите VS Code.
2. В меню слева найдите *Extensions*. Установите расширение *Remote SSH* от Microsoft.
3. В меню слева найдите *Remote explorer*. Нажмите на значок "+" возле SSH (*New remote*).
4. Введите команду `ssh login@hpc.cs.vsu.ru`, где `login` - ваши фамилия_ио **латиницей** (в точном соответствии с вашим заявлением).
5. Нажмите на "➔" (*Connect in current window*). Согласитесь с тем, что вы подключаетесь к устаревшей версии ОС. Выберите первый предложенный путь к файлам конфигурации SSH.
6. В меню слева выберите *Explorer*. Затем *Open folder*. Нажмите ОК, соглашаясь перейти по стандартному пути вида `/lustre/home/login`
7. *Yes, I trust the authors.*
8. Вы подключились к суперкомпьютеру. Уже сейчас вы можете редактировать файлы кода, создавать папки и т.п. Кроме того, можно скачивать файлы на локальный компьютер (*ПКМ* ➔ *Download...*), а также загружать их на сервер, перетаскивая мышью из проводника в VS Code.
9. Нажмите по рабочей папке правой кнопкой мыши, выберите *Open in Integrated terminal*. Откроется приглашение ко вводу вида  
`login@head:~> ` Теперь можно полноценно работать на кластере, загружать нужные модули и ставить задачи в очередь исполнения.

## <a id="files">Часть 2. Как обмениваться файлами</a>
Для того, чтобы запускать код на кластере, необходимо как-то его туда поместить. Первый и самый очевидный способ - создать нужный файл сразу на сервере. Однако это не всегда удобно, поэтому далее рассмотрим основные способы файлообмена.  
**Примечание:** в случае работы через *Visual Studio Code* вы уже имеете необходимые для этого инструменты.

### <a id="linux_caja">На Linux через проводник</a>
1. Откройте файловый менеджер. К примеру, caja.
2. В меню *File* найдите опцию *Connect to server...*
3. Server: hpc.cs.vsu.ru  
   Port: 22  
   Type: SSH  
   Folder: /home/фамилия_ио  
   User Name: фамилия_ио  
   Password: не заполняется

   Здесь ваши фамилия_ио указываются **латиницей** (в точном соответствии с вашим заявлением). При желании можно поставить галочку "Add bookmark" и добавить закладку для последующего быстрого подключения.
4. Нажимайте *Connect*. При успешном подключении вы попадёте в свою папку и увидите несколько служебных папок и файлов, среди которых `.config`, `.ssh`, `.bash_profile`, `.bashrc` и другие.
5. Теперь вы можете работать с вашей папкой на суперкомпьютере так, будто она является обычной папкой на локальной системе. Можно свободно редактировать и переносить файлы между системами.

### <a id="linux_scp">На Linux через терминал</a>
1. Введите команду
```
scp /path/to/sending_file.txt login@hpc.cs.vsu.ru:~/path/to/destination_file.txt
```
Значок `~` обозначает вашу домашнюю папку на кластере. `login` - ваши фамилия_ио **латиницей** (в точном соответствии с вашим заявлением). Файл `sending_file.txt` будет **передан на кластер** и помещён в указанный каталог с именем `destinaton_file.txt`  

2. Введите команду
```
scp login@hpc.cs.vsu.ru:~/path/to/sending_file.txt /path/to/receiving_file.txt
```  
Файл `sending_file.txt` будет **получен от кластера** и помещён в указанный каталог с именем `receiving_file.txt`

### <a id="windows_scp">На Windows через WinSCP</a>
1. Скачайте и установите программу *WinSCP*.
2. Запустите программу *pageant*. Появится значок в трее, нажмите на него ПКМ. Выберите *Add key*. Выберите файл **приватного ключа**.
3. Запустите *WinSCP*. Укажите параметры:  
   Protocol: SFTP  
   Host name: hpc.cs.vsu.ru  
   Port: 22  
   User name: фамилия_ио **латиницей**
4. Нажмите *Login*. В случае успешного подключения вы увидите список папок и файлов вашей домашней папки.
5. Сверху слева можно выбрать диск вашей локальной системы. В этом случае слева будут файлы локлаьного компьютера, а справа - файлы кластера. Можно копировать их в обе стороны через стандартные комбинации `CTRL+C`, `CTRL+V` и т.п. Кроме того, можно открывать текстовые файлы во встроенном редакторе с примитивным интерфейсом.
6. Чтобы отобразить скрытые папки и файлы, перейдите в *Panels* ➔ *Common* ➔ *Show hidden files*.

## <a id="modules">Часть 3. Как загружать модули</a>
Некоторые программы установлены на суперкомпьютере для всех по умолчанию и доступны сразу после входа. К ним относится, к примеру, `gcc-4.8.5`. Однако в стандартный комплект не входят пакеты `OpenMPI` (необходим для работы с параллельными вычислениями через MPI) и `python3`.

### <a id="temp_modules">Для временного использования</a>
1. Исполните `module avail`, чтобы увидеть список всех доступных модулей.
2. Через `module load gcc/13.4.0` в явном виде загрузите `gcc-13.4.0`, чтобы получить расширенный список доступных модулей.
3. Повторно исполните `module avail`. Появились два новых доступных модуля: `openmpi/5.0.8` и `python/3.13.9`.
4. Исполните `module load openmpi/5.0.8`. Теперь нужный модуль загружен и можно работать с MPI. Проверить его доступность можно командой `mpirun --version`
5. При желании можно загрузить модуль `python/3.13.9`. Вместе с ним автоматически загрузятся зависимости, среди которых `sqlite/3.50.4` и `openssl/3.6.0`.
6. Чтобы выгрузить отдельный модуль, выполните `module unload <name>`
7. Чтобы выгрузить все модули, выполните `module purge`
8. Модули будут оставаться загруженными **только до выхода из системы.** После повторного входа нужно будет загружать их заново.

### <a id="const_modules">Для постоянного использования</a>
В случае, если вы пользуетесь одним и тем же модулем постоянно, имеет смысл настроить его загрузку по умолчанию. Для этого:  
1. Исполните `mcedit ~/.bashrc`
2. Перейдите в конец файла и добавьте команды, которые будут исполняться при каждом вашем входе в систему. Например, для модуля `openmpi/5.0.8` введите:
```
# Load OpenMPI
module load gcc/13.4.0 &> /dev/null
module load openmpi/5.0.8 &> /dev/null
```
Перенаправление вывода в /dev/null избавит вас от вывода служебной информации при каждом входе в систему.  
3. Нажмите `F2` для сохранения файла.  
4. Нажмите `F10` для выхода из редактора.  
5. Перезайдите в систему. Проверьте, что команда `mpirun --version` существует и отрабатывает правильно.

## <a id="start_code">Часть 4. Как запускать код</a>
### <a id="c_compilation">Компиляция на Си</a>
1. Чтобы запустить код, написанный на Си, его нужно сначала **скомпилировать** и получить исполняемый выходной файл. Его, в свою очередь, необходимо поставить в **очередь исполнения**, после чего будет получен результат работы программы.  
Синтаксис команды для компиляции выглядит так:  
```
gcc file.c -o prog (-lm) (-fopenmp) (-Wall) (...)
```
Здесь  
    `file.c` - файл с исходным кодом программы  
    `-o` - ключ для задания выходного исполняемого файла (output)  
    `prog` - имя выходного исполняемого файла  
    `-lm` - ключ использования библиотеки `#include <math.h>`  
    `-fopenmp` - ключ использования технологии распараллеливания **OpenMP**  
    `-Wall` - ключ подробного вывода предупреждений (Warnings all)  
    `-lother` - ключ использования любой другой библиотеки (library other)  
Некоторые стандартные библиотеки **не требуют** отдельных ключей для компиляции. К примеру, `stdio.h`, `stdlib.h`, `string.h`, `time.h`.
2. После компиляции необходимо создать скрипт для исполнения программы. Для этого в папке с файлом исходного кода выполните `mcedit start.sh`. Заполните открывшийся файл следующим содержимым:  
```
#!/bin/bash
./prog
```
Здесь `prog` - имя исполняемого файла.  
3. После этого можно ставить задачу [в очередь исполнения](#slurm).

### <a id="c_mpi_compilation">Компиляция на Си с использованием MPI</a>
При использовании технологии распараллеливания MPI (Message Passing Interface) процесс компиляции будет несколько отличаться.
1. Загрузите [модуль OpenMPI](#modules).
2. Исполните команду
```
mpicc file.c -o prog (-lm) (-Wall) (...)
```
Здесь  
    `file.c` - файл с исходным кодом программы  
    `-o` - ключ для задания выходного исполняемого файла (output)  
    `prog` - имя выходного исполняемого файла  
    `-lm` - ключ использования библиотеки `#include <math.h>`  
    `-Wall` - ключ подробного вывода предупреждений (Warnings all)  
    `-lother` - ключ использования любой другой библиотеки (library other)  
Некоторые стандартные библиотеки **не требуют** отдельных ключей для компиляции. К примеру, `stdio.h`, `stdlib.h`, `string.h`, `time.h`. В том числе, к ним относится библиотека `mpi.h`.  
3. Создайте исполняемый скрипт. Для этого в папке с файлом исходного кода выполните `mcedit start_mpi.sh`. Заполните открывшийся файл следующим содержимым: 
```
#!/bin/bash
mpirun ./prog
```
Здесь `prog` - имя исполняемого файла.  
4. После этого можно ставить задачу в [очередь исполнения](#slurm), заменяя `start.sh` на `start_mpi.sh`

### <a id="slurm">Постановка в очередь исполнения</a>
1. Синтаксис команды постановки задачи в очередь выглядит так:  
```
sbatch -n 24 --partition=stu start.sh (arg1) (arg2) (...)
```
Здесь  
    `-n` - ключ задания числа используемых ядер  
    `24` - число используемых ядер (максимум 24 на одном узле)  
    `--partition` - ключ задания очереди  
    `stu` - название студенческой очереди (другие студентам недоступны)  
    `start.sh` - название скрипта, который исполняет программу  
    `arg1 arg2 ...` - опциональные аргументы вашей программы, принимаемые через `argv[]`  
Исполнять задачи через `./prog` в обход очереди исполнения **запрещается!**  
2. Результат выполнения программы будет размещён в текстовом файле вида `slurm-xxxxxx.out`  
3. Для просмотра текущего состояния очереди используется команда `squeue`  
4. Для просмотра состояния всех очередей, в т.ч. недоступных, используется команда `squeue -a`  
5. Для просмотра состояния задач только вашего пользователя используется `mj`  
6. Программное ограничение на длительность исполнения задачи в студенческой очереди составляет **60 минут**. Однако, вообще говоря, учебные программы не должны занимать более **10 минут**.  
7. Если вы видите, что ваша программа исполняется слишком долго, завершите её исполнение вручную, чтобы не занимать ресурсы. Для этого выполните `scancel xxxxxx`, где `xxxxxx` - номер вашей задачи, отображаемый в `squeue`.

## <a id="xeon_phi">Часть 5. Как работать с Xeon Phi</a>
Intel Xeon Phi 7120P - сопроцессоры, построенные на архитектуре Knights Korner.  
Как правило, кодируются аббревиатурой MIC (Many Integrated Core).

Обладают следующими характеристиками:  
    - Базовая тактовая частота: *1.24 ГГц*  
    - Максимальная тактовая частота: *1.33 ГГц*  
    - Объём ОЗУ: *16 ГБ*  
    - Количество ядер: *61 (доступно 60)*  
    - Потоков на ядро: *4*
    
Для сравнения, характеристики вычислительного узла кластера:  
    - Базовая тактовая частота: *2.5 ГГц*  
    - Максимальная тактовая частота: *3.3 ГГц*  
    - Объём ОЗУ: *120 ГБ*  
    - Количество ядер: *24*  
    - Потоков на ядро: *1*

Таким образом, идеальная параллельная программа на 240 потоках Xeon Phi будет работать примерно **в 5 раз быстрее**, чем на 24 потоках стандартного вычислительного узла. Единственный недостаток — небольшое количество оперативной памяти.
Всего на кластере установлено 14 сопроцессоров, из которых 12 доступны для расчётов.

### <a id="access_phi">Получение доступа к сопроцессорам</a>
Физически Xeon Phi расположены на node[1-7] и называются **node[1-7]-mic[0-1]**. Работа с ними, как и с основными вычислительными узлами, возможна только через очередь исполнения SLURM.  
Однако по умолчанию сопроцессоры скрыты. Чтобы разблокировать к ним доступ, нужно выполнить команду:  
```
module load intel/2017
```
На экран будет выведено сообщение `Slurm binaries are redirected to MIC segment.` Теперь вы находитесь в MIC-сегменте кластера и все команды для работы с очередью, включая `squeue`, `sbatch` и `srun`, будут взаимодействовать только с `--partition=mic`. Перечень доступных вычислительных узлов можно посмотреть через `scontrol show nodes`.  
При этом основные вычислительные узлы, включая `--partition=stu`, перестанут быть доступными. Это нормально. Чтобы вернуть к ним доступ, выполните любую из двух команд:
```
module unload intel/2017
module purge
```
![Схема slurm на кластере](https://github.com/Astrogrammer/parallel/blob/main/images/SLURM_HPC.jpg)

### <a id="omp_phi">Компиляция с использованием OpenMP</a>
1. Чтобы скомпилировать код под Xeon Phi, нужно использовать специальный компилятор от Intel под названием `icc`.  
Для получения к нему доступа необходимо загрузить модуль:
```
module load intel/2017
```
2. Синтаксис команды, в целом, похож на `gcc`, но имеет некоторые отличия:
```
icc file.c -o prog_mic -mmic (-qopenmp) (-lm) (-Wall) (...)
```
Здесь  
    `file.c` - файл с исходным кодом программы  
    `-o` - ключ для задания выходного исполняемого файла (output)  
    `prog_mic` - имя выходного исполняемого файла  
    **`-mmic`** - задание конечной архитектуры (в нашем случае, mic)  
    **`-qopenmp`** - ключ использования технологии распараллеливания **OpenMP**  
    `-lm` - ключ использования библиотеки `#include <math.h>`  
    `-Wall` - ключ подробного вывода предупреждений (Warnings all)  
    `-lother` - ключ использования любой другой библиотеки (library other)  
Некоторые стандартные библиотеки **не требуют** отдельных ключей для компиляции. К примеру, `stdio.h`, `stdlib.h`, `string.h`, `time.h`.  
3. Теперь задачу можно ставить в очередь исполнения:
```
sbatch -n 240 (--partition=mic) start.sh (arg1) (arg2) (...)
```
Здесь  
    `-n` - ключ задания числа используемых потоков **(кратно 4 для Xeon Phi)**  
    `240` - число используемых потоков (максимум 240 на одном узле)  
    `--partition` - ключ задания очереди  
    `mic` - название очереди (используется по умолчанию в MIC-сегменте)  
    `start.sh` - название скрипта, который исполняет программу (или `--wrap='./prog_mic'`)  
    `arg1 arg2 ...` - опциональные аргументы вашей программы, принимаемые через `argv[]`  
Исполнять задачи через `./prog_mic` в обход очереди исполнения **запрещается!**  
4. Результат выполнения программы будет размещён в текстовом файле вида `slurm-1xxxxxx.out`.  
    - Во избежание перемешивания номеров заданий с основной очередью отсчёт начинается с 1000000.  
5. Для просмотра текущего состояния очереди используется команда `squeue`   
6. Для просмотра состояния задач только вашего пользователя используется `mj`  
7. Программное ограничение на длительность исполнения задачи в mic-очереди составляет **14 дней**. Однако, вообще говоря, учебные программы не должны занимать более **30 минут**.  
8. Если вы видите, что ваша программа исполняется слишком долго, завершите её исполнение вручную, чтобы не занимать ресурсы. Для этого выполните `scancel 1xxxxxx`, где `1xxxxxx` - номер вашей задачи, отображаемый в `squeue`.

### <a id="mpi_phi">Компиляция с использованием MPI</a>
1. Для компиляции под Xeon Phi с использованием технологии MPI потребуется другой компилятор от Intel - `mpiicc`.
Для получения к нему доступа необходимо загрузить модуль:
```
module load intel/2017
```
2. Синтаксис команды почти целиком повторяет `mpicc`:
```
mpiicc file.c -o prog_mic -mmic (-lm) (-Wall) (...)
```
Здесь  
    `file.c` - файл с исходным кодом программы  
    `-o` - ключ для задания выходного исполняемого файла (output)  
    `prog_mic` - имя выходного исполняемого файла  
    **`-mmic`** - задание конечной архитектуры (в нашем случае, mic)  
    `-lm` - ключ использования библиотеки `#include <math.h>`  
    `-Wall` - ключ подробного вывода предупреждений (Warnings all)  
    `-lother` - ключ использования любой другой библиотеки (library other)  
Некоторые стандартные библиотеки **не требуют** отдельных ключей для компиляции. К примеру, `stdio.h`, `stdlib.h`, `string.h`, `time.h`. В том числе, к ним относится библиотека `mpi.h`.  
3. Создайте исполняемый скрипт. Для этого в папке с файлом исходного кода выполните `mcedit start_mic.sh`. Заполните открывшийся файл следующим содержимым: 
```
#!/bin/bash
mpirun -np $1 ./prog_mic
```
Здесь  
    `-np` - ключ для задания количества процессов  
    `$1` - значение первого входного аргумента  
    `./prog_mic` - исполнение выходного исполняемого файла  
4. Поставьте задачу в очередь исполнения. Для этого введите
```
sbatch -c 240 (--partition=mic) start_mic.sh 240 (arg2) (...)
```
Здесь  
    `-c` - ключ задания числа используемых логических ядер  
    `240` - число используемых логических ядер (максимум 240 на одном узле)  
    `--partition` - ключ задания очереди  
    `mic` - название очереди (используется по умолчанию в MIC-сегменте)  
    `start_mic.sh` - название скрипта, который исполняет программу  
    `240` - аргумент скрипта `start_mic.sh`, определяющий количество процессов в MPI-программе  
    `arg2 ...` - опциональные аргументы вашей программы, принимаемые через `argv[]`  
Исполнять задачи через `mpirun ./prog_mic` в обход очереди исполнения **запрещается!**  
5. Результат выполнения программы будет размещён в текстовом файле вида `slurm-1xxxxxx.out`.  
    - Во избежание перемешивания номеров заданий с основной очередью отсчёт начинается с 1000000.  
6. Для просмотра текущего состояния очереди используется команда `squeue`   
7. Для просмотра состояния задач только вашего пользователя используется `mj`  
8. Программное ограничение на длительность исполнения задачи в mic-очереди составляет **14 дней**. Однако, вообще говоря, учебные программы не должны занимать более **30 минут**.  
9. Если вы видите, что ваша программа исполняется слишком долго, завершите её исполнение вручную, чтобы не занимать ресурсы. Для этого выполните `scancel 1xxxxxx`, где `1xxxxxx` - номер вашей задачи, отображаемый в `squeue`.


## <a id="questions">Часто задаваемые вопросы</a>
### <a id="error_entry">Не удаётся подключиться к hpc.cs.vsu.ru</a>
**Ошибка:** REMOTE HOST IDENTIFICATION HAS CHANGED  
**Причина:** изменились параметры SSH на сервере.  
**Решение:** удалить файл `/home/user/.ssh/known_hosts` (или `C:\Users\user\.ssh\known_hosts`).  
При повторном входе ввести `yes`.  

**Ошибка:** Permission denied (publickey)  
**Причина:** приватный ключ на клиенте не подходит к публичному ключу на сервере.  
*Вариант 1:* неправильно введён логин.  
**Решение:** проверить, что команда имеет вид `ssh ivanov_ii@hpc.cs.vsu.ru`  
(НЕ Ivanov_ii, ivanov_i_i, ivanov_ivanivanovich).  

*Вариант 2:* ключ был сгенерирован через PuTTY и не запущен pageant.  
**Решение:** запустить pageant, нажать правой кнопкой по значку в трее,  
выбрать *Add key* и добавить приватный ключ в формате .ppk.  
Повторить попытку входа через PuTTY.  

*Вариант 3:* ключ сгенерирован на другом компьютере или на другой операционной системе.  
**Решение 1:** войдите через тот компьютер и ОС, с которых был сгенерирован ключ.  
**Решение 2:** сгенерируйте новую пару ключей на текущем устройстве  
и отправьте публичный ключ администратору.  

*Вариант 4:* ключ сгенерирован через PuTTY, а вход осуществляется через терминал или VS Code.  
**Решение 1:** войдите через PuTTY, не забыв запустить pageant.  
**Решение 2:** сгенерируйте новую пару ключей через `ssh keygen -t rsa`  
и отправьте публичный ключ администратору.  

*Вариант 5:* ключ был сгенерирован на MacOS через Wi-Fi ВГУ или ФКН.  
**Решение:** сгенерируйте новую пару ключей через мобильный или домашний интернет  
и отправьте публичный ключ администратору.  

**Ошибка:** The remote host does not meet the prerequisites for running VS Code Server.  
**Причина:** устаревшая ОС на кластере (SLES 12 SP 2).  
**Решение:** установить версию VS Code версии 1.85.2 (подробнее [в отдельном гайде](https://github.com/ded-otshelnik/parallel-programming/blob/main/practices/vscode-remote-linux.md#установка-vscode-подходящей-под-суперкомпьютер)).  

### <a id="error_entry">Можно ли на кластере компилировать программы?</a>
Да, можно и даже нужно. Весь цикл запуска программного кода от компиляции через `gcc` до исполнения через `./prog` должен выполняться на кластере, если планируется её запуск на вычислительных узлах. Если скомпилировать программу на собственном Linux, а затем отправить для исполнения на суперкомпьютер, может возникнуть ошибка, связанная с несовместимостью архитектуры. Не забудьте поставить команду `./prog` [в очередь исполнения](#slurm).

### <a id="error_entry">Можно ли запускать программы вне очереди исполнения?</a>
Вообще говоря, нет. Запуск программ через `./prog` напрямую не допускается. Однако в редких случаях, когда вы уверены, что запуск пройдёт почти мгновенно (потребует доли секунды для исполнения) и всего лишь на 1 ядре, этим правилом можно пренебречь. Тем не менее, гораздо лучше, если во избежание перегрузки головного узла вы воспользуетесь командой `srun ./prog` для запуска программы в интерактивном режиме через очередь исполнения.

### <a id="error_entry">Могу ли я случайно перегрузить кластер?</a>
Строго говоря, нет. Кластер состоит из множества узлов, и основная его мощность заключена в вычислительных узлах, расчёты на которых контролируются системой управления очередью исполнения SLURM. Даже если пользователи потребуют для расчётов в несколько раз больше ресурсов, чем сейчас свободны и доступны для использования, очередь не позволит узлам перегружаться и будет запускать новые задачи только после завершения старых. Используя команды `sbatch` и `srun`, вы гарантированно избежите проблем с перегрузкой.
С другой стороны, головной узел, являющийся точкой входа на кластер для всех пользователей, не предназначен для ресурсоёмких расчётов. Запуская задачи напрямую на нём через `./prog`, вы рискуете стать причиной снижения его производительности. И хотя на всём остальном кластере, включая все вычислительные узлы, это никак не скажется, другим пользователям будут причинены неудобства, в связи с чем вам могут ограничить доступ к суперкомпьютеру.

### <a id="error_entry">Могу ли я случайно что-нибудь сломать на кластере?</a>
Вероятнее всего, нет. Права доступа на кластере настроены для каждого пользователя индивидуально таким образом, чтобы он имел доступ в режиме записи только к своей личной папке, располагающейся в `/home`. Права на чтение есть для более широкого списка каталогов, включающих папку `/common`, хранящую вспомогательные модули и программы, однако этих прав недостаточно для внесения в них каких-либо изменений.  
Случайное или намеренное использование кем-либо из пользователей команд `sudo` или `su`, предоставляющих рут-права, полностью исключено настройками безопасности.  
Тем не менее, "проверять кластер на прочность" заведомо подозрительными командами точно не стоит, если вы заинтересованы в его стабильной и производительной работе.
