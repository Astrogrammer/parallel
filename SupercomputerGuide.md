# Инструкция по использованию суперкомпьютера (кластера) ВГУ
## Содержание
1. [Как подключиться](#connect)
    - [На Linux через терминал](#linux_term)
    - [На Windows через терминал](#windows_term)
    - [На Windows через PuTTY](#windows_putty)
    - [На Linux и Windows через Visual Studio Code](#vs_code)
2. [Как обмениваться файлами](#files)
    - [На Linux через проводник](#linux_caja)
    - [На Linux через терминал](#linux_scp)
    - [На Windows через WinSCP](#windows_scp)
3. [Как загружать модули](#modules)
    - [Для временного использования](#temp_modules)
    - [Для постоянного использования](#const_modules)
4. [Как запускать код](#start_code)
    - [Компиляция на Си](#c_compilation)
    - [Компиляция на Си с использованием MPI](#c_mpi_compilation)
    - [Постановка в очередь исполнения](#slurm)
5. [Как работать с Xeon Phi](#xeon_phi)
    - [Получение доступа к сопроцессорам](#access_phi)
    - [Компиляция с использованием OpenMP](#omp_phi)
    - [Компиляция с использованием MPI](#mpi_phi)

## <a id="connect">Часть 1. Как подключиться</a>
Прежде чем пытаться установить подключение с кластером, убедитесь, что вам предоставлен к нему доступ. Для этого, как минимум, необходимо  
    а) распечатать и заполнить **заявление на регистрацию**;  
    б) сгенерировать и отправить **публичный SSH-ключ**.  

В случае, если оба пункта выполнены, в ближайшее время можно пробовать установить соединение. Для этого есть несколько основных способов.

### <a id="linux_term">На Linux через терминал</a>
1. Исполните команду `ssh login@hpc.cs.vsu.ru`, где `login` - ваши фамилия_ио **латиницей** (в точном соответствии с вашим заявлением).
2. При успешном подключении в конце приветственного текста вы увидите приглашение ко вводу: `login@head:~> `
3. Введите `mc` (Midnight Commander) для открытия файлового менеджера. Через него вы можете видеть все ваши файлы и перемещаться между каталогами. Для выхода нажмите `F10`.
4. Кроме того, доступны следующие опции:  
   `F3` - открыть файл на чтение  
   `F4` - открыть файл на редактирование  
   `F5` - скопировать файл из одного окна в другое  
   `F6` - переместить файл из одного окна в другое  
   `F8` - удалить файл  
5. Все доступные опции подписаны снизу. При открытии файла на запись вы тоже увидите список возможных команд. В частности, `F2` отвечает за сохранение файла.

### <a id="windows_term">На Windows через терминал</a>
1. В случае, если ваш SSH-ключ был сгенерирован командой `ssh-keygen -t rsa`, откройте терминал через `Win+X` ➔ `Терминал`
2. Исполните команду `ssh login@hpc.cs.vsu.ru`, где `login` - ваши фамилия_ио **латиницей** (в точном соответствии с вашим заявлением).
3. При успешном подключении в конце приветственного текста вы увидите приглашение ко вводу: `login@head:~> `
4. Введите `mc` (Midnight Commander) для открытия файлового менеджера. Через него вы можете видеть все ваши файлы и перемещаться между каталогами. Для выхода нажмите `F10`.
5. Кроме того, доступны следующие опции:  
   `F3` - открыть файл на чтение  
   `F4` - открыть файл на редактирование  
   `F5` - скопировать файл из одного окна в другое  
   `F6` - переместить файл из одного окна в другое  
   `F8` - удалить файл  
6. Все доступные опции подписаны снизу. При открытии файла на запись вы тоже увидите список возможных команд. В частности, `F2` отвечает за сохранение файла.

### <a id="windows_putty">На Windows через PuTTY</a>
1. В случае, если ваш SSH-ключ был сгенерирован программой *puttygen*, запустите программу *pageant*. Появится значок в трее, нажмите на него ПКМ. Выберите *Add key*. Выберите файл **приватного ключа**.
2. Запустите программу *PuTTY*. В поле *host* введите `hpc.cs.vsu.ru`. В поле под "Saved Sessions" введите свой логин формата `фамилия_ио` **латиницей** (в точном соответствии с вашим заявлением). Нажмите *Save* для сохранения учётных данных.
3. Нажмите `Open`. Для несохранённого пользователя откроется окно авторизации. Введите свой логин вышеприведённого формата, нажмите *Enter*. **Перед тем, как вводить первый символ**, убедитесь, что используется английская раскладка клавиатуры (иначе может выдать ошибку при верных данных).
4. При успешном подключении в конце приветственного текста вы увидите приглашение ко вводу: `login@head:~> `
5. Введите `mc` (Midnight Commander) для открытия файлового менеджера. Через него вы можете видеть все ваши файлы и перемещаться между каталогами. Для выхода нажмите `F10`.
6. Кроме того, доступны следующие опции:  
   `F3` - открыть файл на чтение  
   `F4` - открыть файл на редактирование  
   `F5` - скопировать файл из одного окна в другое  
   `F6` - переместить файл из одного окна в другое  
   `F8` - удалить файл  
7. Все доступные опции подписаны снизу. При открытии файла на запись вы тоже увидите список возможных команд. В частности, `F2` отвечает за сохранение файла.

### <a id="vs_code">На Linux и Windows через Visual Studio Code</a>
1. Установите VS Code.
2. В меню слева найдите *Extensions*. Установите расширение *Remote SSH* от Microsoft.
3. В меню слева найдите *Remote explorer*. Нажмите на значок "+" возле SSH (*New remote*).
4. Введите команду `ssh login@hpc.cs.vsu.ru`, где `login` - ваши фамилия_ио **латиницей** (в точном соответствии с вашим заявлением).
5. Нажмите на "➔" (*Connect in current window*). Согласитесь с тем, что вы подключаетесь к устаревшей версии ОС. Выберите первый предложенный путь к файлам конфигурации SSH.
6. В меню слева выберите *Explorer*. Затем *Open folder*. Нажмите ОК, соглашаясь перейти по стандартному пути вида `/lustre/home/login`
7. *Yes, I trust the authors.*
8. Вы подключились к суперкомпьютеру. Уже сейчас вы можете редактировать файлы кода, создавать папки и т.п. Кроме того, можно скачивать файлы на локальный компьютер (*ПКМ* ➔ *Download...*), а также загружать их на сервер, перетаскивая мышью из проводника в VS Code.
9. Нажмите по рабочей папке правой кнопкой мыши, выберите *Open in Integrated terminal*. Откроется приглашение ко вводу вида  
`login@head:~> ` Теперь можно полноценно работать на кластере, загружать нужные модули и ставить задачи в очередь исполнения.

## <a id="files">Часть 2. Как обмениваться файлами</a>
Для того, чтобы запускать код на кластере, необходимо как-то его туда поместить. Первый и самый очевидный способ - создать нужный файл сразу на сервере. Однако это не всегда удобно, поэтому далее рассмотрим основные способы файлообмена.  
**Примечание:** в случае работы через *Visual Studio Code* вы уже имеете необходимые для этого инструменты.

### <a id="linux_caja">На Linux через проводник</a>
1. Откройте файловый менеджер. К примеру, caja.
2. В меню *File* найдите опцию *Connect to server...*
3. Server: hpc.cs.vsu.ru  
   Port: 22  
   Type: SSH  
   Folder: /home/фамилия_ио  
   User Name: фамилия_ио  
   Password: не заполняется

   Здесь ваши фамилия_ио указываются **латиницей** (в точном соответствии с вашим заявлением). При желании можно поставить галочку "Add bookmark" и добавить закладку для последующего быстрого подключения.
4. Нажимайте *Connect*. При успешном подключении вы попадёте в свою папку и увидите несколько служебных папок и файлов, среди которых `.config`, `.ssh`, `.bash_profile`, `.bashrc` и другие.
5. Теперь вы можете работать с вашей папкой на суперкомпьютере так, будто она является обычной папкой на локальной системе. Можно свободно редактировать и переносить файлы между системами.

### <a id="linux_scp">На Linux через терминал</a>
1. Введите команду
```
scp /path/to/sending_file.txt login@hpc.cs.vsu.ru:~/path/to/destination_file.txt
```
Значок `~` обозначает вашу домашнюю папку на кластере. `login` - ваши фамилия_ио **латиницей** (в точном соответствии с вашим заявлением). Файл `sending_file.txt` будет **передан на кластер** и помещён в указанный каталог с именем `destinaton_file.txt`  

2. Введите команду
```
scp login@hpc.cs.vsu.ru:~/path/to/sending_file.txt /path/to/receiving_file.txt
```  
Файл `sending_file.txt` будет **получен от кластера** и помещён в указанный каталог с именем `receiving_file.txt`

### <a id="windows_scp">На Windows через WinSCP</a>
1. Скачайте и установите программу *WinSCP*.
2. Запустите программу *pageant*. Появится значок в трее, нажмите на него ПКМ. Выберите *Add key*. Выберите файл **приватного ключа**.
3. Запустите *WinSCP*. Укажите параметры:  
   Protocol: SFTP  
   Host name: hpc.cs.vsu.ru  
   Port: 22  
   User name: фамилия_ио **латиницей**
4. Нажмите *Login*. В случае успешного подключения вы увидите список папок и файлов вашей домашней папки.
5. Сверху слева можно выбрать диск вашей локальной системы. В этом случае слева будут файлы локлаьного компьютера, а справа - файлы кластера. Можно копировать их в обе стороны через стандартные комбинации `CTRL+C`, `CTRL+V` и т.п. Кроме того, можно открывать текстовые файлы во встроенном редакторе с примитивным интерфейсом.
6. Чтобы отобразить скрытые папки и файлы, перейдите в *Panels* ➔ *Common* ➔ *Show hidden files*.

## <a id="modules">Часть 3. Как загружать модули *(версии временно неактуальны)*</a>
Некоторые программы установлены на суперкомпьютере для всех по умолчанию и доступны сразу после входа. К ним относятся `gcc-13.1.0` и `python-3.6.9`. Однако в стандартный комплект не входит пакет `OpenMPI`, необходимый для работы с параллельными вычислениями через MPI. 

### <a id="temp_modules">Для временного использования</a>
1. Исполните `module avail`, чтобы увидеть список всех доступных модулей.
2. Через `module load gcc/13.1.0` в явном виде загрузите `gcc-13.1.0`, чтобы получить расширенный список доступных модулей.
3. Повторно исполните `module avail`. Появились два новых доступных модуля: `openmpi-5.0.6` и `python-3.11.11`.
4. Исполните `module load openmpi-5.0.6`. Теперь нужный модуль загружен и можно работать с MPI. Проверить его доступность можно командой `mpirun --version`
5. При желании можно загрузить модуль `python-3.11.11`. Вместе с ним загрузятся также `openmpi-5.0.6`, `sqlite-3.49.0` и `scalapack-2.2.2`
6. Чтобы выгрузить отдельный модуль, выполните `module unload <name>`
7. Чтобы выгрузить все модули, выполните `module purge`
8. Модули будут оставаться загруженными **только до выхода из системы.** После повторного входа нужно будет загружать их заново.

### <a id="const_modules">Для постоянного использования</a>
В случае, если вы пользуетесь одним и тем же модулем постоянно, имеет смысл настроить его загрузку по умолчанию. Для этого:  
1. Исполните `mcedit ~/.bashrc`
2. Перейдите в конец файла и добавьте команды, которые будут исполняться при каждом вашем входе в систему. Например, для модуля `openmpi/4.0.3` введите:
```
# Load OpenMPI
module load gcc/4.8.5
module load openmpi/4.0.3
```
3. Нажмите `F2` для сохранения файла.
4. Нажмите `F10` для выхода из редактора.
5. Перезайдите в систему. Проверьте, что команда `mpirun --version` существует и отрабатывает правильно.

## <a id="start_code">Часть 4. Как запускать код</a>
### <a id="c_compilation">Компиляция на Си</a>
1. Чтобы запустить код, написанный на Си, его нужно сначала **скомпилировать** и получить исполняемый выходной файл. Его, в свою очередь, необходимо поставить в **очередь исполнения**, после чего будет получен результат работы программы.  
Синтаксис команды для компиляции выглядит так:  
```
gcc file.c -o prog (-lm) (-fopenmp) (-Wall) (...)
```
Здесь  
    `file.c` - файл с исходным кодом программы  
    `-o` - ключ для задания выходного исполняемого файла (output)  
    `prog` - имя выходного исполняемого файла  
    `-lm` - ключ использования библиотеки `#include <math.h>`  
    `-fopenmp` - ключ использования технологии распараллеливания **OpenMP**  
    `-Wall` - ключ подробного вывода предупреждений (Warnings all)  
    `-lother` - ключ использования любой другой библиотеки (library other)  
Некоторые стандартные библиотеки **не требуют** отдельных ключей для компиляции. К примеру, `stdio.h`, `stdlib.h`, `string.h`, `time.h`.
2. После компиляции необходимо создать скрипт для исполнения программы. Для этого в папке с файлом исходного кода выполните `mcedit start.sh`. Заполните открывшийся файл следующим содержимым:  
```
#!/bin/bash
./prog
```
Здесь `prog` - имя исполняемого файла.  
3. После этого можно ставить задачу [в очередь исполнения](#slurm).

### <a id="c_mpi_compilation">Компиляция на Си с использованием MPI</a>
При использовании технологии распараллеливания MPI (Message Passing Interface) процесс компиляции будет несколько отличаться.
1. Загрузите [модуль OpenMPI](#modules).
2. Исполните команду
```
mpicc file.c -o prog (-lm) (-Wall) (...)
```
Здесь  
    `file.c` - файл с исходным кодом программы  
    `-o` - ключ для задания выходного исполняемого файла (output)  
    `prog` - имя выходного исполняемого файла  
    `-lm` - ключ использования библиотеки `#include <math.h>`  
    `-Wall` - ключ подробного вывода предупреждений (Warnings all)  
    `-lother` - ключ использования любой другой библиотеки (library other)  
Некоторые стандартные библиотеки **не требуют** отдельных ключей для компиляции. К примеру, `stdio.h`, `stdlib.h`, `string.h`, `time.h`. В том числе, к ним относится библиотека `mpi.h`.  
3. Создайте исполняемый скрипт. Для этого в папке с файлом исходного кода выполните `mcedit start_mpi.sh`. Заполните открывшийся файл следующим содержимым: 
```
#!/bin/bash
mpirun ./prog
```
Здесь `prog` - имя исполняемого файла.  
4. После этого можно ставить задачу в [очередь исполнения](#slurm), заменяя `start.sh` на `start_mpi.sh`

### <a id="slurm">Постановка в очередь исполнения</a>
1. Синтаксис команды постановки задачи в очередь выглядит так:  
```
sbatch -n 24 --partition=stu start.sh (arg1) (arg2) (...)
```
Здесь  
    `-n` - ключ задания числа используемых ядер  
    `24` - число используемых ядер (максимум 24 на одном узле)  
    `--partition` - ключ задания очереди  
    `stu` - название студенческой очереди (другие студентам недоступны)  
    `start.sh` - название скрипта, который исполняет программу  
    `arg1 arg2 ...` - опциональные аргументы вашей программы, принимаемые через `argv[]`  
Исполнять задачи через `./prog` в обход очереди исполнения **запрещается!**  
2. Результат выполнения программы будет размещён в текстовом файле вида `slurm-xxxxxx.out`  
3. Для просмотра текущего состояния очереди используется команда `squeue`  
4. Для просмотра состояния всех очередей, в т.ч. недоступных, используется команда `squeue -a`  
5. Для просмотра состояния задач только вашего пользователя используется `mj`  
6. Программное ограничение на длительность исполнения задачи в студенческой очереди составляет **60 минут**. Однако, вообще говоря, учебные программы не должны занимать более **10 минут**.  
7. Если вы видите, что ваша программа исполняется слишком долго, завершите её исполнение вручную, чтобы не занимать ресурсы. Для этого выполните `scancel xxxxxx`, где `xxxxxx` - номер вашей задачи, отображаемый в `squeue`.

## <a id="xeon_phi">Часть 5. Как работать с Xeon Phi</a>
Intel Xeon Phi 7120P - сопроцессоры, построенные на архитектуре Knights Korner.  
Как правило, кодируются аббревиатурой MIC (Many Integrated Core).

Обладают следующими характеристиками:  
    - Базовая тактовая частота: *1.24 ГГц*  
    - Максимальная тактовая частота: *1.33 ГГц*  
    - Объём ОЗУ: *16 ГБ*  
    - Количество ядер: *61 (доступно 60)*  
    - Потоков на ядро: *4*
    
Для сравнения, характеристики вычислительного узла кластера:  
    - Базовая тактовая частота: *2.5 ГГц*  
    - Максимальная тактовая частота: *3.3 ГГц*  
    - Объём ОЗУ: *120 ГБ*  
    - Количество ядер: *24*  
    - Потоков на ядро: *1*

Таким образом, идеальная параллельная программа на 240 потоках Xeon Phi будет работать примерно **в 5 раз быстрее**, чем на 24 потоках стандартного вычислительного узла. Единственный недостаток — небольшое количество оперативной памяти.
Всего на кластере установлено 14 сопроцессоров, из которых 12 доступны для расчётов.

### <a id="access_phi">Получение доступа к сопроцессорам</a>
Физически Xeon Phi расположены на node[1-7] и называются **node[1-7]-mic[0-1]**. Работа с ними, как и с основными вычислительными узлами, возможна только через очередь исполнения SLURM.  
Однако по умолчанию сопроцессоры скрыты. Чтобы разблокировать к ним доступ, нужно выполнить команду:  
```
module load intel/2017
```
На экран будет выведено сообщение `Slurm binaries are redirected to MIC segment.` Теперь вы находитесь в MIC-сегменте кластера и все команды для работы с очередью, включая `squeue`, `sbatch` и `srun`, будут взаимодействовать только с `--partition=mic`. Перечень доступных вычислительных узлов можно посмотреть через `scontrol show nodes`.  
При этом основные вычислительные узлы, включая `--partition=stu`, перестанут быть доступными. Это нормально. Чтобы вернуть к ним доступ, выполните любую из двух команд:
```
module unload intel/2017
module purge
```
